# ml.py

questions = {
    "easy": [
        {"q": "What is the main goal of Supervised Learning?", "options": ["Find hidden patterns in unlabeled data", "Learn a mapping from input to output based on labeled pairs", "Maximize a reward through trial and error", "Reduce the dimensionality of data"], "correct": "Learn a mapping from input to output based on labeled pairs"},
        {"q": "In a classification problem, the output variable is:", "options": ["Continuous", "Categorical", "A complex number", "Always binary"], "correct": "Categorical"},
        {"q": "Which of these is an example of Unsupervised Learning?", "options": ["Linear Regression", "Spam Detection", "K-Means Clustering", "Image Classification"], "correct": "K-Means Clustering"},
        {"q": "What does 'K' represent in the K-Nearest Neighbors (KNN) algorithm?", "options": ["Number of clusters", "Number of iterations", "Number of nearest neighbors to consider", "Learning rate"], "correct": "Number of nearest neighbors to consider"},
        {"q": "A model that performs very well on training data but poorly on unseen data is likely:", "options": ["Underfitting", "Overfitting", "Generalized", "Optimal"], "correct": "Overfitting"},
        {"q": "Which evaluation metric is defined as (True Positives) / (Total Predictions)?", "options": ["Recall", "F1-Score", "Accuracy", "Precision"], "correct": "Precision"},
        {"q": "The process of scaling features to have a mean of 0 and a standard deviation of 1 is called:", "options": ["Normalization", "Standardization", "Label Encoding", "Binarization"], "correct": "Standardization"},
        {"q": "What is the common activation function used in the output layer of a binary classifier?", "options": ["ReLU", "Softmax", "Sigmoid", "Tanh"], "correct": "Sigmoid"},
        {"q": "Which of the following is a regression algorithm?", "options": ["Support Vector Machine", "Random Forest", "Linear Regression", "All of the above"], "correct": "All of the above"},
        {"q": "What does 'Mean Squared Error' measure?", "options": ["Classification accuracy", "The average of the squares of the errors", "The number of outliers", "Model complexity"], "correct": "The average of the squares of the errors"}
    ],
    "medium": [
        {"q": "What is the Bias-Variance tradeoff?", "options": ["High bias leads to overfitting, high variance to underfitting", "High bias leads to underfitting, high variance to overfitting", "Bias and Variance should both be maximized", "There is no relationship between them"], "correct": "High bias leads to underfitting, high variance to overfitting"},
        {"q": "In a Decision Tree, what is 'Entropy' used to measure?", "options": ["Tree height", "Impurity or disorder in a group of samples", "Learning rate", "Number of leaf nodes"], "correct": "Impurity or disorder in a group of samples"},
        {"q": "How does Random Forest reduce the risk of overfitting compared to a single Decision Tree?", "options": ["By increasing the depth of trees", "By using a single complex tree", "By averaging the results of many independent trees", "By removing outliers"], "correct": "By averaging the results of many independent trees"},
        {"q": "What is the purpose of a 'Validation Set'?", "options": ["To train the model weights", "To evaluate the final model performance", "To tune hyperparameters and prevent overfitting", "To replace the training set"], "correct": "To tune hyperparameters and prevent overfitting"},
        {"q": "Which technique is used to prevent overfitting by adding a penalty term to the cost function?", "options": ["Regularization", "Backpropagation", "Gradient Descent", "Data Augmentation"], "correct": "Regularization"},
        {"q": "In Logistic Regression, the 'Logit' function is the log of:", "options": ["The probability", "The odds", "The error", "The weights"], "correct": "The odds"},
        {"q": "What is the main advantage of Support Vector Machines (SVM)?", "options": ["They are very fast on large datasets", "They find the hyperplane that maximizes the margin", "They only work for linear data", "They do not require feature scaling"], "correct": "They find the hyperplane that maximizes the margin"},
        {"q": "Which algorithm uses the 'Kernel Trick' to handle non-linear data?", "options": ["Na√Øve Bayes", "K-Means", "SVM", "Linear Regression"], "correct": "SVM"},
        {"q": "What is 'Cross-Validation'?", "options": ["A method to train models faster", "A technique to assess how a model generalizes to an independent dataset", "A way to label data", "A type of neural network"], "correct": "A technique to assess how a model generalizes to an independent dataset"},
        {"q": "In K-Means clustering, how are initial centroids usually chosen?", "options": ["They are always at (0,0)", "Randomly or using K-Means++", "By taking the mean of all data", "By the user manually"], "correct": "Randomly or using K-Means++"}
    ],
    "hard": [
        {"q": "What is the 'Vanishing Gradient' problem in Deep Learning?", "options": ["Gradients become too large during training", "Gradients become very small, preventing weights from updating", "The loss function reaches zero too quickly", "Neurons stop firing completely"], "correct": "Gradients become very small, preventing weights from updating"},
        {"q": "Which of these is a characteristic of 'L1 Regularization' (Lasso)?", "options": ["It adds squared weights to the loss", "It can lead to sparse models by setting some weights to zero", "It is also known as Tikhonov regularization", "It never performs feature selection"], "correct": "It can lead to sparse models by setting some weights to zero"},
        {"q": "What is the 'Curse of Dimensionality'?", "options": ["Higher dimensions make data easier to visualize", "As dimensionality increases, the volume of space increases so fast that data becomes sparse", "Low dimensional data is always noisy", "It only affects supervised learning"], "correct": "As dimensionality increases, the volume of space increases so fast that data becomes sparse"},
        {"q": "In Gradient Boosting, what does each new tree try to predict?", "options": ["The original target values", "The residuals (errors) of the previous ensemble", "The average of all previous trees", "The weights of the features"], "correct": "The residuals (errors) of the previous ensemble"},
        {"q": "What is the primary difference between Bagging and Boosting?", "options": ["Bagging builds models sequentially; Boosting builds them in parallel", "Bagging reduces variance; Boosting primarily reduces bias", "Bagging is only for regression; Boosting is only for classification", "There is no difference"], "correct": "Bagging reduces variance; Boosting primarily reduces bias"},
        {"q": "Principal Component Analysis (PCA) performs dimensionality reduction by:", "options": ["Dropping random features", "Projecting data onto directions of maximum variance", "Selecting the most correlated features", "Using a non-linear kernel"], "correct": "Projecting data onto directions of maximum variance"},
        {"q": "What is the role of the 'Softmax' function in a Multi-class Neural Network?", "options": ["To normalize hidden layers", "To turn raw scores into probabilities that sum to 1", "To act as a dropout layer", "To calculate the final loss"], "correct": "To turn raw scores into probabilities that sum to 1"},
        {"q": "Which of the following is true about 'Stochastic Gradient Descent' (SGD)?", "options": ["It uses the entire dataset for every update", "It updates parameters using only one sample at a time", "It is much slower than Batch Gradient Descent", "It always finds the global minimum"], "correct": "It updates parameters using only one sample at a time"},
        {"q": "A ROC curve plots which two metrics against each other?", "options": ["Precision and Recall", "True Positive Rate and False Positive Rate", "Accuracy and Loss", "Bias and Variance"], "correct": "True Positive Rate and False Positive Rate"},
        {"q": "In Reinforcement Learning, the 'Bellman Equation' is used to find:", "options": ["The optimal policy or value function", "The error in classification", "The distance between clusters", "The principal components"], "correct": "The optimal policy or value function"}
    ]
}